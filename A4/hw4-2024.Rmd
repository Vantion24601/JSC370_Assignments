---
title: "Assignment 04 - HPC and ML 2"
output: html_document
highlight: tango
link-citations: yes
---

## Due Date

April 5, 2024 by 11:59pm.



## HPC

1. Make sure your code is nice! Rewrite the following R functions to make them faster. It is OK (and recommended) to take a look at Stackoverflow.

```{r, eval = FALSE}
# Total row sums
fun1 <- function(mat) {
  n <- nrow(mat)
  ans <- double(n) 
  for (i in 1:n) {
    ans[i] <- sum(mat[i, ])
  }
  ans
}

fun1alt <- function(mat) {
  # YOUR CODE HERE
  rowSums(mat)
}

# Cumulative sum by row
fun2 <- function(mat) {
  n <- nrow(mat)
  k <- ncol(mat)
  ans <- mat
  for (i in 1:n) {
    for (j in 2:k) {
      ans[i,j] <- mat[i, j] + ans[i, j - 1]
    }
  }
  ans
}

fun2alt <- function(mat) {
  # YOUR CODE HERE
  t(apply(mat, 1, cumsum))
}


# Use the data with this code
set.seed(2315)
dat <- matrix(rnorm(200 * 100), nrow = 200)

# Test for the first
microbenchmark::microbenchmark(
  fun1(dat),
  fun1alt(dat), unit = "relative", check = "equivalent"
)

# Test for the second
microbenchmark::microbenchmark(
  fun2(dat),
  fun2alt(dat), unit = "relative", check = "equivalent"
)
```

The last argument, check = “equivalent”, is included to make sure that the functions return the same result.

2. Make things run faster with parallel computing. The following function allows simulating Pi:

```{r, eval = FALSE}
sim_pi <- function(n = 1000, i = NULL) {
  p <- matrix(runif(n*2), ncol = 2)
  mean(rowSums(p^2) < 1) * 4
}

# Here is an example of the run
set.seed(156)
sim_pi(1000) # 3.132
```

In order to get accurate estimates, we can run this function multiple times, with the following code:

```{r, eval = FALSE}
# This runs the simulation a 4,000 times, each with 10,000 points
set.seed(1231)
system.time({
  ans <- unlist(lapply(1:4000, sim_pi, n = 10000))
  print(mean(ans))
})
```

Rewrite the previous code using `parLapply()` to make it run faster. Make sure you set the seed using `clusterSetRNGStream()`:

```{r, eval = FALSE}
# YOUR CODE HERE
ncpus <- 4
cl <- makePSOCKcluster(ncpus)
clusterSetRNGStream(cl, 123)

system.time({
  # YOUR CODE HERE
  ans <- unlist(parLapply(cl, rep(10000, 4000), sim_pi))
  print(mean(ans))
  # YOUR CODE HERE
  stopCluster(cl)
})
```

## Machine Learning
For this part we will use the `hitters` dataset, which consists of data for 332 major league baseball players. The data are [here](https://github.com/JSC370/JSC370-2024/tree/main/data/hitters). The main goal is to predict players' salaries (variable `Salary`) based on the features in the data. To do so you will replicate many of the concepts in lab 10 (trees, bagging, random forest, boosting and xgboost). Please split the data into training and testing sets (70-30) and use the same sets for all questions.

```{r message=FALSE,warning=FALSE}
library(tidyverse)
library(rpart)
library(rpart.plot)
library(randomForest)
library(gbm)
library(xgboost)
library(caret)
```


```{r}
hitters <- read.csv("hitters.csv", stringsAsFactors = TRUE)
hitters <- na.omit(hitters)
n <- nrow(hitters)
```


```{r}
train_size<-sample(1:n, size = n * 0.7)
train <- hitters[train_size,]
test <- hitters[-train_size,]
```



### 1. Fit a regression tree to predict `Salary`, and appropriately prune it based on the optimal complexity parameter. Summarize.


```{r}
treefit<-rpart(Salary~ ., method = 'anova', control = list(cp=0), data = train) 
```


```{r}
plotcp(treefit)
```


```{r}
treefit$cptable
```

```{r}
optimalcp<-0.149
treepruned<-prune(treefit, cp=optimalcp)

rpart.plot(treepruned)
```

```{r}
summary(treepruned)
```


### 2. Predict `Salary` using bagging, construct a variable importance plot.


```{r}
n_features <- dim(train)[2] - 1

bagging <- randomForest(Salary ~ . , 
                        data = train, 
                        mtry = n_features,
                        na.action = na.omit)
varImpPlot(bagging)
```

```{r}
importance(bagging)
```


### 3. Repeat 2. using random forest. 

```{r}
rf <- randomForest(Salary ~ . , 
                   data = train, 
                   na.action = na.omit)
varImpPlot(rf)

```

```{r}
importance(rf)
```



### 4. Perform boosting with 1,000 trees for a range of values of the shrinkage parameter $\lambda$. Produce a plot with different shrinkage values on the x-axis and corresponding training set MSE on the y-axis. Construct a variable importance plot.


```{r}
lambdas <- c(0.01,0.1,0.3)
mses <- numeric(3)
names(mses) <- lambdas

for (lambda in lambdas) {
  boost <- gbm(
    Salary ~ ., 
    data = train, 
    distribution = "gaussian",
    n.trees = 1000,
    shrinkage = lambda,
    cv.folds = 5
  )
  pred <- predict(boost, train, n.trees = 1000, type = "response")
  mses[as.character(lambda)] <- mean((train$Salary - pred)^2)
}
```

```{r}
mse_df <- data.frame(shrinkage = lambdas, mse = mses)
ggplot(mse_df, aes(x = shrinkage, y = mse)) +
  geom_line() +
  geom_point() +
  theme_minimal() +
  labs(x = "Shrinkage", y = "Training Set MSE")
```


```{r}
summary_gbm <- summary.gbm(boost)
```

```{r}
summary_gbm
```


5. Repeat 4. using XGBoost (set up as a grid search on eta, can also grid search on other parameters).


```{r results='hide'}
grid <- expand.grid(eta = c(0.01,0.1,0.3), 
                    nrounds = (1:10)*50,
                    max_depth = c(1, 3, 5, 7),
                    gamma = 0,
                    colsample_bytree = 0.6,
                    subsample = 1,
                    min_child_weight = 1)

cv_control <- trainControl(method = "cv", number = 10, search = "grid")

xgb <- caret::train(
  Salary ~ .,
  data = train,
  method = "xgbTree",
  trControl = cv_control,
  tuneGrid = grid,
  metric = "RMSE",
  verbosity = 0
)

print(xgb)
```



```{r}
varimp <- varImp(xgb, scale = FALSE)
plot(varimp)
```



### 6. Calculate the test MSE for each method and compare. Which approach has the best performance?


```{r}
pred_treepruned <- predict(treepruned, test)
mse_treepruned <- mean((test$Salary - pred_treepruned)^2)

pred_bagging <- predict(bagging, test)
mse_bagging <- mean((test$Salary - pred_bagging)^2)

pred_rf <- predict(rf, test)
mse_rf <- mean((test$Salary - pred_rf)^2)

pred_boost <- predict(boost, test, n.trees = 1000, type = "response")
mse_boost <- mean((test$Salary - pred_boost)^2)

pred_xgb <- predict(xgb, newdata = test)
mse_xgb <- mean((test$Salary - pred_xgb)^2)

mse_comparison <- data.frame(
  Method = c("Regression Tree", "Bagging", "Random Forest", "Boosting", "XGBoost"),
  MSE = c(mse_treepruned, mse_bagging, mse_rf, mse_boost, mse_xgb)
)

mse_comparison
```
XGBoost has the best performance with lowest test MSE.

7. Compare the variable importance across 2 through 4 (bagging, rf, boosting, XGBoost).

Bagging: The top three features are CRBI, CHits, and Walks. Bagging seems to give more importance to the cumulative statistics.

Random Forest: The top three features are CHits, CRBI, and CRuns, which shows that similar to bagging, rf also emphasizes career statistics but also includes career runs, which points to the player's ability to score.

Boosting: The top three features are CRuns, Walks and PutOuts, which is slightly different and appears to give more weight to defensive features.

XGBoost: The top three features here are CHits, CRBI, and Hits, similar to bagging and rf.

All the four models weight similar features highly(mostly career features), and features like League, New League and Division have very limited importance to all the four models.



